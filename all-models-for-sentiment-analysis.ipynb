{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de902de1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-11T03:48:39.509376Z",
     "iopub.status.busy": "2022-03-11T03:48:39.508521Z",
     "iopub.status.idle": "2022-03-11T03:48:39.542687Z",
     "shell.execute_reply": "2022-03-11T03:48:39.543317Z",
     "shell.execute_reply.started": "2022-03-10T15:44:54.280762Z"
    },
    "papermill": {
     "duration": 0.101712,
     "end_time": "2022-03-11T03:48:39.543631",
     "exception": false,
     "start_time": "2022-03-11T03:48:39.441919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/fast-text-embeddings-without-subwords/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\n",
      "/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec\n",
      "/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\n",
      "/kaggle/input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\n",
      "/kaggle/input/glove2word2vec/glove_w2v.txt\n",
      "/kaggle/input/financial-sentiment-analysis/data.csv\n",
      "/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423162e",
   "metadata": {
    "papermill": {
     "duration": 0.048765,
     "end_time": "2022-03-11T03:48:39.643677",
     "exception": false,
     "start_time": "2022-03-11T03:48:39.594912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**This notebook will give step by step approach to NLP problem from basic models to deep learning models** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2938f39",
   "metadata": {
    "papermill": {
     "duration": 0.049129,
     "end_time": "2022-03-11T03:48:39.741886",
     "exception": false,
     "start_time": "2022-03-11T03:48:39.692757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cf1ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:48:39.848214Z",
     "iopub.status.busy": "2022-03-11T03:48:39.847003Z",
     "iopub.status.idle": "2022-03-11T03:49:51.226032Z",
     "shell.execute_reply": "2022-03-11T03:49:51.225439Z",
     "shell.execute_reply.started": "2022-03-10T15:44:54.342502Z"
    },
    "papermill": {
     "duration": 71.431967,
     "end_time": "2022-03-11T03:49:51.226188",
     "exception": false,
     "start_time": "2022-03-11T03:48:39.794221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\r\n",
      "  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "     |████████████████████████████████| 4.9 MB 805 kB/s            \r\n",
      "\u001b[?25hCollecting tensorflow<2.9,>=2.8.0\r\n",
      "  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\r\n",
      "     |████████████████████████████████| 497.5 MB 21 kB/s              \r\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_text) (0.12.0)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.13.3)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\r\n",
      "     |████████████████████████████████| 2.1 MB 35.2 MB/s            \r\n",
      "\u001b[?25hCollecting keras<2.9,>=2.8.0rc0\r\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "     |████████████████████████████████| 1.4 MB 35.3 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.20.3)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.43.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.12)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (59.5.0)\r\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\r\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\r\n",
      "     |████████████████████████████████| 462 kB 42.4 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.1.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (4.0.1)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.0)\r\n",
      "Collecting libclang>=9.0.1\r\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\r\n",
      "     |████████████████████████████████| 14.5 MB 28.5 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (0.15.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (1.16.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.19.1)\r\n",
      "Collecting tensorboard<2.9,>=2.8\r\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\r\n",
      "     |████████████████████████████████| 5.8 MB 36.4 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.37.0)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.5.2)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.26.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.3.6)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.35.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.6.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.8.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.8)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.2.7)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.3.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (4.10.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (1.26.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (2.0.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.6.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text) (3.1.1)\r\n",
      "Installing collected packages: tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, libclang, keras, tensorflow, tensorflow-text\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.6.0\r\n",
      "    Uninstalling tensorboard-2.6.0:\r\n",
      "      Successfully uninstalled tensorboard-2.6.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.6.0\r\n",
      "    Uninstalling keras-2.6.0:\r\n",
      "      Successfully uninstalled keras-2.6.0\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.6.2\r\n",
      "    Uninstalling tensorflow-2.6.2:\r\n",
      "      Successfully uninstalled tensorflow-2.6.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\r\n",
      "tfx-bsl 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\r\n",
      "tfx-bsl 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\r\n",
      "tfx-bsl 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires numpy<1.20,>=1.16, but you have numpy 1.20.3 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\r\n",
      "tensorflow-transform 1.5.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,<2.8,>=1.15.2, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.8.0 which is incompatible.\r\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.24.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed keras-2.8.0 libclang-13.0.0 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4877fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:49:51.799168Z",
     "iopub.status.busy": "2022-03-11T03:49:51.797990Z",
     "iopub.status.idle": "2022-03-11T03:49:57.024807Z",
     "shell.execute_reply": "2022-03-11T03:49:57.024293Z",
     "shell.execute_reply.started": "2022-03-10T15:45:03.057243Z"
    },
    "papermill": {
     "duration": 5.519649,
     "end_time": "2022-03-11T03:49:57.024951",
     "exception": false,
     "start_time": "2022-03-11T03:49:51.505302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e237dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:49:57.541528Z",
     "iopub.status.busy": "2022-03-11T03:49:57.539011Z",
     "iopub.status.idle": "2022-03-11T03:49:57.588628Z",
     "shell.execute_reply": "2022-03-11T03:49:57.587976Z",
     "shell.execute_reply.started": "2022-03-10T15:45:10.245676Z"
    },
    "papermill": {
     "duration": 0.305494,
     "end_time": "2022-03-11T03:49:57.588785",
     "exception": false,
     "start_time": "2022-03-11T03:49:57.283291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/financial-sentiment-analysis/data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddbe343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:49:58.272442Z",
     "iopub.status.busy": "2022-03-11T03:49:58.271718Z",
     "iopub.status.idle": "2022-03-11T03:49:58.282224Z",
     "shell.execute_reply": "2022-03-11T03:49:58.282950Z",
     "shell.execute_reply.started": "2022-03-10T15:45:10.298552Z"
    },
    "papermill": {
     "duration": 0.443161,
     "end_time": "2022-03-11T03:49:58.283177",
     "exception": false,
     "start_time": "2022-03-11T03:49:57.840016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mapping to the sentiment column \n",
    "\n",
    "dicto = {'positive': 1, 'neutral': 0 , 'negative': -1}\n",
    "\n",
    "df.Sentiment = df.Sentiment.map(dicto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ddc0b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:49:58.943748Z",
     "iopub.status.busy": "2022-03-11T03:49:58.942992Z",
     "iopub.status.idle": "2022-03-11T03:50:00.543583Z",
     "shell.execute_reply": "2022-03-11T03:50:00.543006Z",
     "shell.execute_reply.started": "2022-03-10T15:45:10.315278Z"
    },
    "papermill": {
     "duration": 1.914087,
     "end_time": "2022-03-11T03:50:00.543735",
     "exception": false,
     "start_time": "2022-03-11T03:49:58.629648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5842x13215 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 111586 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctv = CountVectorizer(tokenizer= word_tokenize, token_pattern=None)\n",
    "\n",
    "ctv.fit_transform(df['Sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87b6b2",
   "metadata": {
    "papermill": {
     "duration": 0.248529,
     "end_time": "2022-03-11T03:50:01.041195",
     "exception": false,
     "start_time": "2022-03-11T03:50:00.792666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model-1 Logistic + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2686274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:01.560596Z",
     "iopub.status.busy": "2022-03-11T03:50:01.558496Z",
     "iopub.status.idle": "2022-03-11T03:50:17.524660Z",
     "shell.execute_reply": "2022-03-11T03:50:17.523690Z",
     "shell.execute_reply.started": "2022-03-10T15:45:11.912739Z"
    },
    "papermill": {
     "duration": 16.231523,
     "end_time": "2022-03-11T03:50:17.524924",
     "exception": false,
     "start_time": "2022-03-11T03:50:01.293401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.6826347305389222\n",
      "recall score: 0.6826347305389222\n",
      "\n",
      "precision score: 0.6852010265183918\n",
      "recall score: 0.6852010265183918\n",
      "\n",
      "precision score: 0.6986301369863014\n",
      "recall score: 0.6986301369863014\n",
      "\n",
      "precision score: 0.6678082191780822\n",
      "recall score: 0.6678082191780822\n",
      "\n",
      "precision score: 0.6909246575342466\n",
      "recall score: 0.6909246575342466\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.73      0.75       370\n",
      "          -1       0.30      0.22      0.26       172\n",
      "           0       0.72      0.80      0.76       626\n",
      "\n",
      "    accuracy                           0.69      1168\n",
      "   macro avg       0.60      0.58      0.59      1168\n",
      "weighted avg       0.68      0.69      0.68      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Baseline model let's start with a logistic regression model since it is the \n",
    "#fastest for high dimensional sparse data\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop= True)\n",
    "\n",
    "y = df.Sentiment.values\n",
    "\n",
    "# Intitiate kfold class from model_selection module\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f\n",
    "\n",
    "count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "# we go over the folds\n",
    "for fold_ in range(5):\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    count_vec.fit(train_df.Sentence)\n",
    "    \n",
    "    xtrain = count_vec.transform(train_df.Sentence)\n",
    "    xtest = count_vec.transform(test_df.Sentence)\n",
    "    \n",
    "    model = linear_model.LogisticRegression(solver = 'liblinear')\n",
    "    \n",
    "    model.fit(xtrain,train_df.Sentiment)\n",
    "    \n",
    "    preds = model.predict(xtest)\n",
    "    \n",
    "    accuracy_precision = precision_score(test_df.Sentiment, preds, average='micro')\n",
    "    accuracy_recall = recall_score(test_df.Sentiment, preds, average = 'micro')\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "print(classification_report(test_df.Sentiment, preds, labels=[1, -1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf18279",
   "metadata": {
    "papermill": {
     "duration": 0.251598,
     "end_time": "2022-03-11T03:50:18.111774",
     "exception": false,
     "start_time": "2022-03-11T03:50:17.860176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that labels **1,0 have 70 percent** score but label **-1 score is pretty low**, hence we can conclude that we need to go on modelling untill these scores get better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84de03a",
   "metadata": {
    "papermill": {
     "duration": 0.254364,
     "end_time": "2022-03-11T03:50:18.618584",
     "exception": false,
     "start_time": "2022-03-11T03:50:18.364220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2 NaiveBayes + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d21b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:19.158839Z",
     "iopub.status.busy": "2022-03-11T03:50:19.157913Z",
     "iopub.status.idle": "2022-03-11T03:50:27.795913Z",
     "shell.execute_reply": "2022-03-11T03:50:27.796341Z",
     "shell.execute_reply.started": "2022-03-10T15:45:27.936756Z"
    },
    "papermill": {
     "duration": 8.898949,
     "end_time": "2022-03-11T03:50:27.796522",
     "exception": false,
     "start_time": "2022-03-11T03:50:18.897573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.7108639863130881\n",
      "\n",
      "precision score: 0.6629597946963216\n",
      "\n",
      "precision score: 0.7071917808219178\n",
      "\n",
      "precision score: 0.6943493150684932\n",
      "\n",
      "precision score: 0.714041095890411\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.67      0.71       370\n",
      "          -1       0.46      0.33      0.38       172\n",
      "           0       0.73      0.85      0.79       626\n",
      "\n",
      "    accuracy                           0.71      1168\n",
      "   macro avg       0.65      0.61      0.63      1168\n",
      "weighted avg       0.70      0.71      0.70      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets try with NaiveBayes model \n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for fold_ ,(t_,v_) in enumerate(kf.split(X= df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = fold_\n",
    "    \n",
    "for f in range(5):\n",
    "    df_train = df[df.kfold != f].reset_index(drop=True)\n",
    "    df_test = df[df.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "    \n",
    "    x_train = count_vec.fit_transform(df_train.Sentence)\n",
    "    x_test = count_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    \n",
    "    model.fit(x_train, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "    \n",
    "    #accuracy_recall = recall_score(df_test.Sentiment, y_pred)\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    #print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b7e23",
   "metadata": {
    "papermill": {
     "duration": 0.252459,
     "end_time": "2022-03-11T03:50:28.305638",
     "exception": false,
     "start_time": "2022-03-11T03:50:28.053179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It is evident that the for **label -1 the the metrics values improved** so this model will be the new baseline model for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd5b04",
   "metadata": {
    "papermill": {
     "duration": 0.266044,
     "end_time": "2022-03-11T03:50:28.823400",
     "exception": false,
     "start_time": "2022-03-11T03:50:28.557356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model-3 Naive Bayes with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16de5b66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:29.418563Z",
     "iopub.status.busy": "2022-03-11T03:50:29.417624Z",
     "iopub.status.idle": "2022-03-11T03:50:37.418009Z",
     "shell.execute_reply": "2022-03-11T03:50:37.418486Z",
     "shell.execute_reply.started": "2022-03-10T15:45:36.138779Z"
    },
    "papermill": {
     "duration": 8.274982,
     "end_time": "2022-03-11T03:50:37.418638",
     "exception": false,
     "start_time": "2022-03-11T03:50:29.143656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.6526946107784432\n",
      "\n",
      "precision score: 0.6561163387510693\n",
      "\n",
      "precision score: 0.6575342465753424\n",
      "\n",
      "precision score: 0.6318493150684932\n",
      "\n",
      "precision score: 0.6575342465753424\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.41      0.52       370\n",
      "          -1       1.00      0.03      0.06       172\n",
      "           0       0.64      0.98      0.78       626\n",
      "\n",
      "    accuracy                           0.66      1168\n",
      "   macro avg       0.78      0.47      0.45      1168\n",
      "weighted avg       0.72      0.66      0.59      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now lets try with TF-IDF vectorizer instead of bag of words to MultinomialNB().\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for f_,(t_,v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f_\n",
    "    \n",
    "for fold in range(5):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop = True)\n",
    "    df_test = df[df.kfold == fold].reset_index(drop = True)\n",
    "    \n",
    "    tf_vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "    \n",
    "    xtrain = tf_vec.fit_transform(df_train.Sentence)\n",
    "    xtest = tf_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    \n",
    "    model.fit(xtrain, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(xtest)\n",
    "    \n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "\n",
    "    print('precision score:', accuracy_precision)\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6378035",
   "metadata": {
    "papermill": {
     "duration": 0.253333,
     "end_time": "2022-03-11T03:50:37.930246",
     "exception": false,
     "start_time": "2022-03-11T03:50:37.676913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model-4 Naive Bayes with Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e68acad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:38.448095Z",
     "iopub.status.busy": "2022-03-11T03:50:38.446946Z",
     "iopub.status.idle": "2022-03-11T03:50:50.071160Z",
     "shell.execute_reply": "2022-03-11T03:50:50.071834Z",
     "shell.execute_reply.started": "2022-03-10T15:45:44.644027Z"
    },
    "papermill": {
     "duration": 11.889321,
     "end_time": "2022-03-11T03:50:50.072049",
     "exception": false,
     "start_time": "2022-03-11T03:50:38.182728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.6578272027373824\n",
      "\n",
      "precision score: 0.6441402908468776\n",
      "\n",
      "precision score: 0.6669520547945206\n",
      "\n",
      "precision score: 0.660958904109589\n",
      "\n",
      "precision score: 0.6549657534246576\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.66      0.71       370\n",
      "          -1       0.20      0.14      0.17       172\n",
      "           0       0.68      0.80      0.73       626\n",
      "\n",
      "    accuracy                           0.65      1168\n",
      "   macro avg       0.55      0.53      0.54      1168\n",
      "weighted avg       0.64      0.65      0.64      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now to the baseline bag of word Naivebayes model lets apply Ngrams  and compare the results.\n",
    "\n",
    "# Lets try with NaiveBayes model \n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for fold_ ,(t_,v_) in enumerate(kf.split(X= df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = fold_\n",
    "    \n",
    "for f in range(5):\n",
    "    df_train = df[df.kfold != f].reset_index(drop=True)\n",
    "    df_test = df[df.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None, ngram_range = (1,3))\n",
    "    \n",
    "    x_train = count_vec.fit_transform(df_train.Sentence)\n",
    "    x_test = count_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    \n",
    "    model.fit(x_train, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "    \n",
    "    #accuracy_recall = recall_score(df_test.Sentiment, y_pred)\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    #print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c657310",
   "metadata": {
    "papermill": {
     "duration": 0.25714,
     "end_time": "2022-03-11T03:50:50.591239",
     "exception": false,
     "start_time": "2022-03-11T03:50:50.334099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So the **ngrams has even decreased the metrics** value.Hence it is evident that the baseline model is still the best one till now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc7a61",
   "metadata": {
    "papermill": {
     "duration": 0.266049,
     "end_time": "2022-03-11T03:50:51.113917",
     "exception": false,
     "start_time": "2022-03-11T03:50:50.847868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce24da4",
   "metadata": {
    "papermill": {
     "duration": 0.2593,
     "end_time": "2022-03-11T03:50:51.631242",
     "exception": false,
     "start_time": "2022-03-11T03:50:51.371942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will **not remove stopwords** in this because it might change the context of the sentence.\n",
    "\n",
    "e.g **\"He is not a good person\"** will be changed into **\" 'He' , 'good', 'person'\"** which is the complete opposite of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b99fd0fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:52.154178Z",
     "iopub.status.busy": "2022-03-11T03:50:52.153288Z",
     "iopub.status.idle": "2022-03-11T03:50:52.155465Z",
     "shell.execute_reply": "2022-03-11T03:50:52.155846Z",
     "shell.execute_reply.started": "2022-03-10T15:45:55.891597Z"
    },
    "papermill": {
     "duration": 0.26417,
     "end_time": "2022-03-11T03:50:52.155998",
     "exception": false,
     "start_time": "2022-03-11T03:50:51.891828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now lets do some cleaning on the text data and apply it to baseline model and compare the accuracies.\n",
    "\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    \n",
    "    text = word_tokenize(text)\n",
    "    text = [re.sub('[^A-Za-z]+', '', word) for word in text]\n",
    "    text = [word.lower() for word in text if word.isalpha()]\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f9a4294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:52.668875Z",
     "iopub.status.busy": "2022-03-11T03:50:52.668007Z",
     "iopub.status.idle": "2022-03-11T03:50:52.676558Z",
     "shell.execute_reply": "2022-03-11T03:50:52.676942Z",
     "shell.execute_reply.started": "2022-03-10T15:45:55.899618Z"
    },
    "papermill": {
     "duration": 0.268821,
     "end_time": "2022-03-11T03:50:52.677125",
     "exception": false,
     "start_time": "2022-03-11T03:50:52.408304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'good', 'person']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'He is not a good person'\n",
    "token_text = word_tokenize(text)\n",
    "[ word for word in token_text if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73374710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:53.428374Z",
     "iopub.status.busy": "2022-03-11T03:50:53.427518Z",
     "iopub.status.idle": "2022-03-11T03:50:57.548710Z",
     "shell.execute_reply": "2022-03-11T03:50:57.547690Z",
     "shell.execute_reply.started": "2022-03-10T15:45:55.916193Z"
    },
    "papermill": {
     "duration": 4.556976,
     "end_time": "2022-03-11T03:50:57.548884",
     "exception": false,
     "start_time": "2022-03-11T03:50:52.991908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.Sentence = df.Sentence.apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca941816",
   "metadata": {
    "papermill": {
     "duration": 0.261037,
     "end_time": "2022-03-11T03:50:58.087122",
     "exception": false,
     "start_time": "2022-03-11T03:50:57.826085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 5 cleaned text Naive Bayes BOW and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b90fcc66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:50:58.941160Z",
     "iopub.status.busy": "2022-03-11T03:50:58.939978Z",
     "iopub.status.idle": "2022-03-11T03:51:05.949863Z",
     "shell.execute_reply": "2022-03-11T03:51:05.950737Z",
     "shell.execute_reply.started": "2022-03-10T15:46:00.299076Z"
    },
    "papermill": {
     "duration": 7.517756,
     "end_time": "2022-03-11T03:51:05.951019",
     "exception": false,
     "start_time": "2022-03-11T03:50:58.433263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.7151411462788708\n",
      "\n",
      "precision score: 0.7057313943541489\n",
      "\n",
      "precision score: 0.714041095890411\n",
      "\n",
      "precision score: 0.7071917808219178\n",
      "\n",
      "precision score: 0.6926369863013698\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.64      0.68       370\n",
      "          -1       0.43      0.33      0.37       172\n",
      "           0       0.72      0.83      0.77       626\n",
      "\n",
      "    accuracy                           0.69      1168\n",
      "   macro avg       0.63      0.60      0.61      1168\n",
      "weighted avg       0.68      0.69      0.68      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now lets try this on our baseline MultinomialNB bagofwords model\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for fold_ ,(t_,v_) in enumerate(kf.split(X= df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = fold_\n",
    "    \n",
    "for f in range(5):\n",
    "    df_train = df[df.kfold != f].reset_index(drop=True)\n",
    "    df_test = df[df.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "    \n",
    "    x_train = count_vec.fit_transform(df_train.Sentence)\n",
    "    x_test = count_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = MultinomialNB()\n",
    "    \n",
    "    model.fit(x_train, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "    \n",
    "    #accuracy_recall = recall_score(df_test.Sentiment, y_pred)\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    #print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0effe2",
   "metadata": {
    "papermill": {
     "duration": 0.253718,
     "end_time": "2022-03-11T03:51:06.462148",
     "exception": false,
     "start_time": "2022-03-11T03:51:06.208430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hurray!!! after some text cleaning **we beat our baseline model by small margin** hence the above model will be our new baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078967c5",
   "metadata": {
    "papermill": {
     "duration": 0.256885,
     "end_time": "2022-03-11T03:51:06.997104",
     "exception": false,
     "start_time": "2022-03-11T03:51:06.740219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now lets go into **word embeddings** \n",
    "\n",
    "In the above models each word token is converted into integer tokens by (BOW and TFIDF) now lets convert these **integer tokens into vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e408060",
   "metadata": {
    "papermill": {
     "duration": 0.254677,
     "end_time": "2022-03-11T03:51:07.531608",
     "exception": false,
     "start_time": "2022-03-11T03:51:07.276931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 6 Fastext vector with Naive Bayes Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f04d9cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:51:08.067727Z",
     "iopub.status.busy": "2022-03-11T03:51:08.067096Z",
     "iopub.status.idle": "2022-03-11T03:58:43.368467Z",
     "shell.execute_reply": "2022-03-11T03:58:43.369333Z",
     "shell.execute_reply.started": "2022-03-10T15:46:06.88273Z"
    },
    "papermill": {
     "duration": 455.572141,
     "end_time": "2022-03-11T03:58:43.369676",
     "exception": false,
     "start_time": "2022-03-11T03:51:07.797535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n",
      "creating sentence vectors\n",
      "fold:  0\n",
      "precision and recall scores: 0.5372112917023096 0.5372112917023096\n",
      "\n",
      "fold:  1\n",
      "precision and recall scores: 0.5363558597091531 0.5363558597091531\n",
      "\n",
      "fold:  2\n",
      "precision and recall scores: 0.5368150684931506 0.5368150684931506\n",
      "\n",
      "fold:  3\n",
      "precision and recall scores: 0.5368150684931506 0.5368150684931506\n",
      "\n",
      "fold:  4\n",
      "precision and recall scores: 0.5368150684931506 0.5368150684931506\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.00      0.01       370\n",
      "          -1       0.00      0.00      0.00       172\n",
      "           0       0.54      1.00      0.70       626\n",
      "\n",
      "    accuracy                           0.54      1168\n",
      "   macro avg       0.51      0.33      0.23      1168\n",
      "weighted avg       0.60      0.54      0.38      1168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# In this model we will use fastText vectors and also convert each word vector in sentence vector.\n",
    "\n",
    "#The code is taken from https://fasttext.cc/docs/en/english-vectors.html, this code splits each vector by \n",
    "# space and return for more info go through the above linl\n",
    " \n",
    "import io\n",
    "\n",
    "def sentence_to_vec(s, embedding_dict, tokenizer):\n",
    "    \n",
    "    words = tokenizer(s)\n",
    "    \n",
    "    embedding_list = []\n",
    "    for w in words:\n",
    "        \n",
    "        if w in embedding_dict:\n",
    "            embedding_list.append(embedding_dict[w])\n",
    "            \n",
    "    # if we dont have any vectors, then return zeros\n",
    "    if len(embedding_list) == 0:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    #convert list of embeddings into an array\n",
    "    embedding_list = np.array(embedding_list)\n",
    "    print\n",
    "    #calculate sum over axis = 0 (This will convert the word vectorrs to sentence vectors)\n",
    "    v = embedding_list.sum(axis=0)\n",
    "    \n",
    "    #return normalized vector\n",
    "    return v\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df.drop('kfold', axis=1, inplace = True)\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "#load embeddings into memory\n",
    "print(\"Loading embeddings\")\n",
    "embeddings = KeyedVectors.load_word2vec_format('/kaggle/input/fast-text-embeddings-without-subwords/crawl-300d-2M.vec/crawl-300d-2M.vec')\n",
    "    \n",
    "#create sentence embeddings\n",
    "print(\"creating sentence vectors\")\n",
    "vectors = []\n",
    "for sentence in df.Sentence.values:\n",
    "    vectors.append(         \n",
    "       sentence_to_vec(s = sentence, embedding_dict= embeddings, tokenizer = word_tokenize)\n",
    "    )\n",
    "vectors = np.array(vectors)\n",
    "    \n",
    "for fold_,(train_, valid_) in enumerate(kf.split(X=df, y=y)):\n",
    "        print(\"fold: \", fold_)\n",
    "        \n",
    "        xtrain = vectors[train_,:]\n",
    "        ytrain = y[train_]\n",
    "        \n",
    "        xtest = vectors[valid_,:]\n",
    "        ytest = y[valid_]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        xscaled_train = scaler.fit_transform(xtrain)\n",
    "        xscaled_test = scaler.transform(xtest)\n",
    "        \n",
    "        model = MultinomialNB()\n",
    "    \n",
    "        model.fit(xscaled_train,ytrain)\n",
    "    \n",
    "        y_pred = model.predict(xscaled_test)\n",
    "    \n",
    "        pres_score = precision_score(ytest, y_pred, average='micro')\n",
    "    \n",
    "        rec_score = recall_score(ytest, y_pred, average='micro')\n",
    "    \n",
    "        print('precision and recall scores:', pres_score, rec_score)\n",
    "        print(\"\")\n",
    "    \n",
    "print(classification_report(ytest, y_pred, labels=[1,-1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71a1bf",
   "metadata": {
    "papermill": {
     "duration": 0.259277,
     "end_time": "2022-03-11T03:58:43.958822",
     "exception": false,
     "start_time": "2022-03-11T03:58:43.699545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Whoa! It is our worst performing model so far! What should we do now? Should we ask muppets to save us? No! we will keep on trying with other models and methods.Thats what data scientists do!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55280f14",
   "metadata": {
    "papermill": {
     "duration": 0.274445,
     "end_time": "2022-03-11T03:58:44.490349",
     "exception": false,
     "start_time": "2022-03-11T03:58:44.215904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 7 - Glove vectors with Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e590f7a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:58:45.065503Z",
     "iopub.status.busy": "2022-03-11T03:58:45.064565Z",
     "iopub.status.idle": "2022-03-11T03:58:45.066481Z",
     "shell.execute_reply": "2022-03-11T03:58:45.066889Z",
     "shell.execute_reply.started": "2022-03-10T15:53:37.754033Z"
    },
    "papermill": {
     "duration": 0.281585,
     "end_time": "2022-03-11T03:58:45.067036",
     "exception": false,
     "start_time": "2022-03-11T03:58:44.785451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            w_line = line.split()\n",
    "            curr_word = w_line[0]\n",
    "            word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27fc54f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:58:45.591445Z",
     "iopub.status.busy": "2022-03-11T03:58:45.584202Z",
     "iopub.status.idle": "2022-03-11T03:58:45.593990Z",
     "shell.execute_reply": "2022-03-11T03:58:45.593557Z",
     "shell.execute_reply.started": "2022-03-10T15:53:37.765756Z"
    },
    "papermill": {
     "duration": 0.272271,
     "end_time": "2022-03-11T03:58:45.594144",
     "exception": false,
     "start_time": "2022-03-11T03:58:45.321873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "    def __init__(self, model):\n",
    "        print(\"Loading in word vectors...\")\n",
    "        self.word_vectors = model\n",
    "        print(\"Finished loading in word vectors\")\n",
    "    \n",
    "    def fit(self, data):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, data):\n",
    "        # determine the dimensionality of vectors\n",
    "        v = self.word_vectors.get_vector('king')\n",
    "        self.D = v.shape[0]\n",
    "\n",
    "        X = np.zeros((len(data), self.D))\n",
    "        n = 0\n",
    "        emptycount = 0\n",
    "    \n",
    "        for sentence in data:\n",
    "            tokens = sentence.split()\n",
    "            vecs = []\n",
    "            m = 0\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    vec = self.word_vectors.get_vector(word)\n",
    "                    vecs.append(vec)\n",
    "                    m += 1\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            if len(vecs) > 0:\n",
    "                vecs = np.array(vecs)\n",
    "                X[n] = vecs.mean(axis=0)\n",
    "            else:\n",
    "                emptycount += 1\n",
    "            n += 1\n",
    "        print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "        return X\n",
    "\n",
    "\n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf7bc6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:58:46.335130Z",
     "iopub.status.busy": "2022-03-11T03:58:46.333965Z",
     "iopub.status.idle": "2022-03-11T03:59:49.454798Z",
     "shell.execute_reply": "2022-03-11T03:59:49.454305Z",
     "shell.execute_reply.started": "2022-03-10T15:53:37.785189Z"
    },
    "papermill": {
     "duration": 63.604268,
     "end_time": "2022-03-11T03:59:49.454939",
     "exception": false,
     "start_time": "2022-03-11T03:58:45.850671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_output_file = '/kaggle/input/glove2word2vec/glove_w2v.txt'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary = False)\n",
    "\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832f6838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:59:49.982992Z",
     "iopub.status.busy": "2022-03-11T03:59:49.981980Z",
     "iopub.status.idle": "2022-03-11T03:59:50.343822Z",
     "shell.execute_reply": "2022-03-11T03:59:50.344318Z",
     "shell.execute_reply.started": "2022-03-10T15:54:40.587002Z"
    },
    "papermill": {
     "duration": 0.630519,
     "end_time": "2022-03-11T03:59:50.344492",
     "exception": false,
     "start_time": "2022-03-11T03:59:49.713973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n",
      "Numer of samples with no words found: 0 / 4673\n",
      "Numer of samples with no words found: 0 / 1169\n"
     ]
    }
   ],
   "source": [
    "#we create a Vectorizer object that will help us to transform our reviews to vectors, a numerical representation. \n",
    "#Then we can use those vectors to feed our classifier.\n",
    "\n",
    "vectorizer = Word2VecVectorizer(model)\n",
    "\n",
    "X_train = vectorizer.fit_transform(Xtrain)\n",
    "y_train = ytrain\n",
    "\n",
    "X_test = vectorizer.transform(Xtest)\n",
    "y_test = ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9185cb5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T03:59:50.878212Z",
     "iopub.status.busy": "2022-03-11T03:59:50.877087Z",
     "iopub.status.idle": "2022-03-11T04:01:28.164554Z",
     "shell.execute_reply": "2022-03-11T04:01:28.165232Z",
     "shell.execute_reply.started": "2022-03-10T15:54:40.925105Z"
    },
    "papermill": {
     "duration": 97.553424,
     "end_time": "2022-03-11T04:01:28.165452",
     "exception": false,
     "start_time": "2022-03-11T03:59:50.612028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.67      0.70       359\n",
      "          -1       0.41      0.20      0.27       181\n",
      "           0       0.71      0.86      0.78       629\n",
      "\n",
      "    accuracy                           0.70      1169\n",
      "   macro avg       0.62      0.58      0.58      1169\n",
      "weighted avg       0.68      0.70      0.68      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters to tune\n",
    "parameters = { \n",
    "    'C': [1.0, 10],\n",
    "    'gamma': [1, 'auto', 'scale']\n",
    "}\n",
    "\n",
    "model = GridSearchCV(SVC(kernel='rbf'), parameters, cv=5, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "#model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred, labels=[1,-1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2c246",
   "metadata": {
    "papermill": {
     "duration": 0.258938,
     "end_time": "2022-03-11T04:01:28.682826",
     "exception": false,
     "start_time": "2022-03-11T04:01:28.423888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Model 5 is still our best performing model, lets do some hyperparameter tuning before we move on to deep learning models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db0f91",
   "metadata": {
    "papermill": {
     "duration": 0.25817,
     "end_time": "2022-03-11T04:01:29.229959",
     "exception": false,
     "start_time": "2022-03-11T04:01:28.971789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model # Hyperparameter tuning best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6b07f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:01:29.817147Z",
     "iopub.status.busy": "2022-03-11T04:01:29.796730Z",
     "iopub.status.idle": "2022-03-11T04:01:31.759610Z",
     "shell.execute_reply": "2022-03-11T04:01:31.759138Z",
     "shell.execute_reply.started": "2022-03-10T15:56:16.429703Z"
    },
    "papermill": {
     "duration": 2.267605,
     "end_time": "2022-03-11T04:01:31.759805",
     "exception": false,
     "start_time": "2022-03-11T04:01:29.492200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'alpha': 1.6}\n",
      "Best score:  0.7113974594841629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n",
    "\n",
    "X_train = count_vec.fit_transform(df.Sentence)\n",
    "\n",
    "parameters = {'alpha': [0.001,0.01,0.1,0.2,0.3,0.5,0.7,1,1.5,1.6,1.8,10,100]}\n",
    "\n",
    "grid_search = MultinomialNB()\n",
    "\n",
    "grid_search = GridSearchCV(grid_search, parameters, cv=5, scoring = 'f1_micro', n_jobs = -1)\n",
    "\n",
    "grid_result = grid_search.fit(X_train, df.Sentiment)\n",
    "\n",
    "print('Best params: ', grid_result.best_params_)\n",
    "print('Best score: ', grid_result.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e93a2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:01:32.339087Z",
     "iopub.status.busy": "2022-03-11T04:01:32.328939Z",
     "iopub.status.idle": "2022-03-11T04:09:03.561098Z",
     "shell.execute_reply": "2022-03-11T04:09:03.561770Z",
     "shell.execute_reply.started": "2022-03-10T15:56:18.04935Z"
    },
    "papermill": {
     "duration": 451.535146,
     "end_time": "2022-03-11T04:09:03.561970",
     "exception": false,
     "start_time": "2022-03-11T04:01:32.026824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.536, total=   8.8s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.536, total=   8.2s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.536, total=   8.4s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.564, total=   7.6s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.558, total=   7.9s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.536, total=   5.8s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.3s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.536, total=   5.2s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.474, total=   8.6s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.472, total=   8.6s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.473, total=   9.3s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.624, total=   7.8s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.622, total=   8.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.703, total=   6.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.680, total=   5.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.679, total=   6.6s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.536, total=   6.1s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.3s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.472, total=   8.8s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.470, total=   8.9s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.640, total=   8.4s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.634, total=   8.9s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.624, total=   8.3s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.683, total=   6.5s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.705, total=   6.3s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.684, total=   5.9s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.676, total=   6.2s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.676, total=   5.4s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.536, total=   6.0s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.536, total=   6.1s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.472, total=   9.5s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.470, total=   8.7s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.639, total=   8.2s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.634, total=   8.7s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.629, total=   8.9s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.663, total=   6.6s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.670, total=   7.1s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.689, total=   5.7s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.699, total=   5.4s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.687, total=   6.1s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.689, total=   5.5s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.695, total=   5.8s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.477, total=   8.7s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.473, total=   8.7s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.474, total=   9.7s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.630, total=   8.5s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.623, total=   8.7s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.672, total=   7.3s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.673, total=   6.9s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.664, total=   7.0s[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.536, total=   9.0s\n",
      "[CV] C=0.1, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=0.1, gamma=1, kernel=rbf, score=0.536, total=   8.1s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.550, total=   7.6s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.559, total=   7.6s\n",
      "[CV] C=0.1, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=0.1, gamma=0.1, kernel=rbf, score=0.565, total=   7.8s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=0.1, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=0.1, gamma=0.01, kernel=rbf, score=0.536, total=   6.6s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=0.1, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=0.1, gamma=0.001, kernel=rbf, score=0.536, total=   5.8s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.3s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.536, total=   5.2s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=0.1, gamma=0.0001, kernel=rbf, score=0.536, total=   5.6s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.473, total=   8.2s\n",
      "[CV] C=1, gamma=1, kernel=rbf ........................................\n",
      "[CV] ............ C=1, gamma=1, kernel=rbf, score=0.470, total=   9.1s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.633, total=   8.9s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.633, total=   8.2s\n",
      "[CV] C=1, gamma=0.1, kernel=rbf ......................................\n",
      "[CV] .......... C=1, gamma=0.1, kernel=rbf, score=0.629, total=   8.0s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.681, total=   5.8s\n",
      "[CV] C=1, gamma=0.01, kernel=rbf .....................................\n",
      "[CV] ......... C=1, gamma=0.01, kernel=rbf, score=0.693, total=   6.5s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.536, total=   6.1s\n",
      "[CV] C=1, gamma=0.001, kernel=rbf ....................................\n",
      "[CV] ........ C=1, gamma=0.001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.1s\n",
      "[CV] C=1, gamma=0.0001, kernel=rbf ...................................\n",
      "[CV] ....... C=1, gamma=0.0001, kernel=rbf, score=0.536, total=   6.3s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.477, total=   8.9s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.473, total=   8.7s\n",
      "[CV] C=10, gamma=1, kernel=rbf .......................................\n",
      "[CV] ........... C=10, gamma=1, kernel=rbf, score=0.474, total=   8.9s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.630, total=   9.1s\n",
      "[CV] C=10, gamma=0.1, kernel=rbf .....................................\n",
      "[CV] ......... C=10, gamma=0.1, kernel=rbf, score=0.629, total=   8.2s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.686, total=   6.5s\n",
      "[CV] C=10, gamma=0.01, kernel=rbf ....................................\n",
      "[CV] ........ C=10, gamma=0.01, kernel=rbf, score=0.695, total=   6.3s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.707, total=   5.6s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.689, total=   6.4s\n",
      "[CV] C=10, gamma=0.001, kernel=rbf ...................................\n",
      "[CV] ....... C=10, gamma=0.001, kernel=rbf, score=0.697, total=   5.5s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.536, total=   6.2s\n",
      "[CV] C=10, gamma=0.0001, kernel=rbf ..................................\n",
      "[CV] ...... C=10, gamma=0.0001, kernel=rbf, score=0.536, total=   5.9s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.477, total=   8.9s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.473, total=   9.4s\n",
      "[CV] C=100, gamma=1, kernel=rbf ......................................\n",
      "[CV] .......... C=100, gamma=1, kernel=rbf, score=0.474, total=   8.8s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.630, total=   8.3s\n",
      "[CV] C=100, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ........ C=100, gamma=0.1, kernel=rbf, score=0.623, total=   8.6s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.668, total=   7.3s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.677, total=   7.1s\n",
      "[CV] C=100, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] ....... C=100, gamma=0.01, kernel=rbf, score=0.666, total=   6.8s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.684, total=   5.6s\n",
      "[CV] C=100, gamma=0.001, kernel=rbf ..................................\n",
      "[CV] ...... C=100, gamma=0.001, kernel=rbf, score=0.688, total=   5.3s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.706, total=   6.4s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.680, total=   5.4s\n",
      "[CV] C=100, gamma=0.0001, kernel=rbf .................................\n",
      "[CV] ..... C=100, gamma=0.0001, kernel=rbf, score=0.679, total=   5.7s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.472, total=   8.9s\n",
      "[CV] C=1000, gamma=1, kernel=rbf .....................................\n",
      "[CV] ......... C=1000, gamma=1, kernel=rbf, score=0.470, total=   9.2s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.639, total=   8.6s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.634, total=   8.6s\n",
      "[CV] C=1000, gamma=0.1, kernel=rbf ...................................\n",
      "[CV] ....... C=1000, gamma=0.1, kernel=rbf, score=0.629, total=   8.5s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.655, total=   7.3s\n",
      "[CV] C=1000, gamma=0.01, kernel=rbf ..................................\n",
      "[CV] ...... C=1000, gamma=0.01, kernel=rbf, score=0.655, total=   6.9s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.660, total=   6.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best score:  0.6906901461265337\n"
     ]
    }
   ],
   "source": [
    "# Lets also check svm hyperparameters using pipelines\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n",
    "\n",
    "X_train = count_vec.fit_transform(df.Sentence)\n",
    "\n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    " \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=5, scoring = 'f1_micro', n_jobs = -1)\n",
    " \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, df.Sentiment)\n",
    "\n",
    "print('Best params: ', grid.best_params_)\n",
    "print('Best score: ', grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26981fe9",
   "metadata": {
    "papermill": {
     "duration": 0.259923,
     "end_time": "2022-03-11T04:09:04.079279",
     "exception": false,
     "start_time": "2022-03-11T04:09:03.819356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets use grid search for hyperparameterization.Eventhough its computationally expensive it may perform very good on our model and dataset lets see!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1603714",
   "metadata": {
    "papermill": {
     "duration": 0.257267,
     "end_time": "2022-03-11T04:09:04.595249",
     "exception": false,
     "start_time": "2022-03-11T04:09:04.337982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 8 Best basline model+hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d349afca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:09:05.123100Z",
     "iopub.status.busy": "2022-03-11T04:09:05.122514Z",
     "iopub.status.idle": "2022-03-11T04:09:11.707236Z",
     "shell.execute_reply": "2022-03-11T04:09:11.707954Z",
     "shell.execute_reply.started": "2022-03-10T16:03:43.822325Z"
    },
    "papermill": {
     "duration": 6.854454,
     "end_time": "2022-03-11T04:09:11.708183",
     "exception": false,
     "start_time": "2022-03-11T04:09:04.853729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.7262617621899059\n",
      "\n",
      "precision score: 0.7117194183062446\n",
      "\n",
      "precision score: 0.7148972602739726\n",
      "\n",
      "precision score: 0.711472602739726\n",
      "\n",
      "precision score: 0.699486301369863\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.61      0.67       370\n",
      "          -1       0.45      0.19      0.27       172\n",
      "           0       0.71      0.89      0.79       626\n",
      "\n",
      "    accuracy                           0.70      1168\n",
      "   macro avg       0.63      0.57      0.58      1168\n",
      "weighted avg       0.68      0.70      0.67      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now lets try this on our baseline MultinomialNB bagofwords model\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for fold_ ,(t_,v_) in enumerate(kf.split(X= df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = fold_\n",
    "    \n",
    "for f in range(5):\n",
    "    df_train = df[df.kfold != f].reset_index(drop=True)\n",
    "    df_test = df[df.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "    \n",
    "    x_train = count_vec.fit_transform(df_train.Sentence)\n",
    "    x_test = count_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = MultinomialNB(alpha = 1.6)\n",
    "    \n",
    "    model.fit(x_train, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "    \n",
    "    #accuracy_recall = recall_score(df_test.Sentiment, y_pred)\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    #print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "154ede3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:09:12.238907Z",
     "iopub.status.busy": "2022-03-11T04:09:12.237813Z",
     "iopub.status.idle": "2022-03-11T04:09:38.058568Z",
     "shell.execute_reply": "2022-03-11T04:09:38.059045Z",
     "shell.execute_reply.started": "2022-03-10T16:03:50.63496Z"
    },
    "papermill": {
     "duration": 26.087442,
     "end_time": "2022-03-11T04:09:38.059246",
     "exception": false,
     "start_time": "2022-03-11T04:09:11.971804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score: 0.6826347305389222\n",
      "\n",
      "precision score: 0.7014542343883661\n",
      "\n",
      "precision score: 0.6892123287671232\n",
      "\n",
      "precision score: 0.6952054794520548\n",
      "\n",
      "precision score: 0.6866438356164384\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.66      0.71       370\n",
      "          -1       0.29      0.20      0.24       172\n",
      "           0       0.72      0.83      0.77       626\n",
      "\n",
      "    accuracy                           0.69      1168\n",
      "   macro avg       0.59      0.57      0.57      1168\n",
      "weighted avg       0.67      0.69      0.67      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.Sentiment.values\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "for fold_ ,(t_,v_) in enumerate(kf.split(X= df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = fold_\n",
    "    \n",
    "for f in range(5):\n",
    "    df_train = df[df.kfold != f].reset_index(drop=True)\n",
    "    df_test = df[df.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern=None)\n",
    "    \n",
    "    x_train = count_vec.fit_transform(df_train.Sentence)\n",
    "    x_test = count_vec.transform(df_test.Sentence)\n",
    "    \n",
    "    model = SVC(C= 100, gamma =  0.001, kernel = 'rbf')\n",
    "    \n",
    "    model.fit(x_train, df_train.Sentiment)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    accuracy_precision = precision_score(df_test.Sentiment, y_pred, average = 'micro')\n",
    "    \n",
    "    #accuracy_recall = recall_score(df_test.Sentiment, y_pred)\n",
    "    \n",
    "    print('precision score:', accuracy_precision)\n",
    "    #print('recall score:', accuracy_recall)\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "print(classification_report(df_test.Sentiment, y_pred, labels=[1,-1,0]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0360e2c",
   "metadata": {
    "papermill": {
     "duration": 0.264766,
     "end_time": "2022-03-11T04:09:38.596968",
     "exception": false,
     "start_time": "2022-03-11T04:09:38.332202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The model has performed worser than the previous above model, we can also create different pipelines of different machine learning models and compare them with both BOW and tfidf models**\n",
    "\n",
    "**More models including deep learning models are yet to come**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e743e1",
   "metadata": {
    "papermill": {
     "duration": 0.261663,
     "end_time": "2022-03-11T04:09:39.135911",
     "exception": false,
     "start_time": "2022-03-11T04:09:38.874248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model- 8 LSTM using glove word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fe820bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:09:39.670229Z",
     "iopub.status.busy": "2022-03-11T04:09:39.669523Z",
     "iopub.status.idle": "2022-03-11T04:10:45.196416Z",
     "shell.execute_reply": "2022-03-11T04:10:45.196876Z",
     "shell.execute_reply.started": "2022-03-10T16:04:16.569626Z"
    },
    "papermill": {
     "duration": 65.800348,
     "end_time": "2022-03-11T04:10:45.197093",
     "exception": false,
     "start_time": "2022-03-11T04:09:39.396745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading pretrained google news word2vec embedding 300D\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_pretrained = KeyedVectors.load_word2vec_format(\"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\",binary=True)\n",
    "word2vec_pretrained_dict = dict(zip(word2vec_pretrained.key_to_index.keys(), \n",
    "                                    word2vec_pretrained.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "525404b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:45.787509Z",
     "iopub.status.busy": "2022-03-11T04:10:45.751668Z",
     "iopub.status.idle": "2022-03-11T04:10:47.573849Z",
     "shell.execute_reply": "2022-03-11T04:10:47.573377Z",
     "shell.execute_reply.started": "2022-03-10T16:05:24.414877Z"
    },
    "papermill": {
     "duration": 2.116549,
     "end_time": "2022-03-11T04:10:47.573982",
     "exception": false,
     "start_time": "2022-03-11T04:10:45.457433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Sentence'] = df['Sentence'].apply(process_text)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df.Sentence, df.Sentiment, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "345098ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:48.095763Z",
     "iopub.status.busy": "2022-03-11T04:10:48.095019Z",
     "iopub.status.idle": "2022-03-11T04:10:48.098104Z",
     "shell.execute_reply": "2022-03-11T04:10:48.097674Z",
     "shell.execute_reply.started": "2022-03-10T16:05:26.279361Z"
    },
    "papermill": {
     "duration": 0.266858,
     "end_time": "2022-03-11T04:10:48.098239",
     "exception": false,
     "start_time": "2022-03-11T04:10:47.831381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_enc = np_utils.to_categorical(y_train, 3)\n",
    "y_test_enc = np_utils.to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a21ffbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:48.688372Z",
     "iopub.status.busy": "2022-03-11T04:10:48.687360Z",
     "iopub.status.idle": "2022-03-11T04:10:49.011731Z",
     "shell.execute_reply": "2022-03-11T04:10:49.012179Z",
     "shell.execute_reply.started": "2022-03-10T16:05:26.289778Z"
    },
    "papermill": {
     "duration": 0.635925,
     "end_time": "2022-03-11T04:10:49.012350",
     "exception": false,
     "start_time": "2022-03-11T04:10:48.376425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token = tf.keras.preprocessing.text.Tokenizer(num_words=None)\n",
    "\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "xtrain_seq = token.texts_to_sequences(X_train)\n",
    "xtest_seq = token.texts_to_sequences(X_test)\n",
    "\n",
    "#zero pad sequences \n",
    "\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq)\n",
    "xtest_pad = sequence.pad_sequences(xtest_seq)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1f19b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:49.538509Z",
     "iopub.status.busy": "2022-03-11T04:10:49.537781Z",
     "iopub.status.idle": "2022-03-11T04:10:49.569424Z",
     "shell.execute_reply": "2022-03-11T04:10:49.568928Z",
     "shell.execute_reply.started": "2022-03-10T16:05:26.631181Z"
    },
    "papermill": {
     "duration": 0.300568,
     "end_time": "2022-03-11T04:10:49.569552",
     "exception": false,
     "start_time": "2022-03-11T04:10:49.268984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create embedding matrix for words that we have in dataset\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 300))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = word2vec_pretrained_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1291c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:50.095883Z",
     "iopub.status.busy": "2022-03-11T04:10:50.095258Z",
     "iopub.status.idle": "2022-03-11T04:10:50.546868Z",
     "shell.execute_reply": "2022-03-11T04:10:50.545899Z",
     "shell.execute_reply.started": "2022-03-10T16:05:26.666205Z"
    },
    "papermill": {
     "duration": 0.720019,
     "end_time": "2022-03-11T04:10:50.547015",
     "exception": false,
     "start_time": "2022-03-11T04:10:49.826996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 04:10:50.220617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-03-11 04:10:50.231497: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# A simple LSTM with two dense layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1,300,weights=[embedding_matrix], trainable = False))\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "\n",
    "model.add(Dense(1024 , activation = 'relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation = 'relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = [tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486792b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:10:51.092485Z",
     "iopub.status.busy": "2022-03-11T04:10:51.091616Z",
     "iopub.status.idle": "2022-03-11T04:48:17.075351Z",
     "shell.execute_reply": "2022-03-11T04:48:17.076093Z",
     "shell.execute_reply.started": "2022-03-10T16:05:27.15055Z"
    },
    "papermill": {
     "duration": 2246.265507,
     "end_time": "2022-03-11T04:48:17.076290",
     "exception": false,
     "start_time": "2022-03-11T04:10:50.810783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 04:10:54.470798: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n",
      "2022-03-11 04:10:54.497224: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n",
      "2022-03-11 04:10:56.104002: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n",
      "2022-03-11 04:10:56.201103: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 58s - loss: 1.1072 - precision: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-11 04:10:57.676219: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 30720000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 27s 2s/step - loss: 1.0037 - precision: 0.5660 - val_loss: 0.9320 - val_precision: 0.7268\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.9198 - precision: 0.6690 - val_loss: 0.9513 - val_precision: 0.6989\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8866 - precision: 0.7359 - val_loss: 0.9019 - val_precision: 0.6613\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8712 - precision: 0.7196 - val_loss: 0.8393 - val_precision: 0.7084\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.8487 - precision: 0.7258 - val_loss: 0.8016 - val_precision: 0.7295\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8044 - precision: 0.7233 - val_loss: 0.7638 - val_precision: 0.7433\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7811 - precision: 0.7299 - val_loss: 0.7607 - val_precision: 0.7686\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7627 - precision: 0.7364\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.650, total=   6.2s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.652, total=   5.8s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.693, total=   5.4s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.688, total=   6.1s\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7627 - precision: 0.7364 - val_loss: 0.7463 - val_precision: 0.7179\n",
      "Epoch 9/100\n",
      " 1/10 [==>...........................] - ETA: 20s - loss: 0.8347 - precision: 0.6751\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.660, total=   6.1s\n",
      "[CV] C=1000, gamma=0.001, kernel=rbf .................................\n",
      "[CV] ..... C=1000, gamma=0.001, kernel=rbf, score=0.657, total=   5.8s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.689, total=   5.3s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.697, total=   6.2s\n",
      "[CV] C=1000, gamma=0.0001, kernel=rbf ................................\n",
      "[CV] .... C=1000, gamma=0.0001, kernel=rbf, score=0.688, total=   3.7s\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7782 - precision: 0.7231 - val_loss: 0.7448 - val_precision: 0.7600\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7502 - precision: 0.7448 - val_loss: 0.6872 - val_precision: 0.7800\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7297 - precision: 0.7315 - val_loss: 0.6953 - val_precision: 0.8089\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7194 - precision: 0.7498 - val_loss: 0.6737 - val_precision: 0.7983\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7248 - precision: 0.7488 - val_loss: 0.6703 - val_precision: 0.7832\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7086 - precision: 0.7421 - val_loss: 0.6615 - val_precision: 0.8009\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7026 - precision: 0.7571 - val_loss: 0.6978 - val_precision: 0.7613\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6873 - precision: 0.7573 - val_loss: 0.6732 - val_precision: 0.7980\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6851 - precision: 0.7646 - val_loss: 0.6609 - val_precision: 0.7900\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6596 - precision: 0.7714 - val_loss: 0.6267 - val_precision: 0.7808\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6496 - precision: 0.7636 - val_loss: 0.6244 - val_precision: 0.7965\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6631 - precision: 0.7669 - val_loss: 0.6313 - val_precision: 0.8067\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6549 - precision: 0.7660 - val_loss: 0.6303 - val_precision: 0.8151\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6394 - precision: 0.7789 - val_loss: 0.6039 - val_precision: 0.7989\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6248 - precision: 0.7765 - val_loss: 0.5984 - val_precision: 0.8083\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6237 - precision: 0.7705 - val_loss: 0.6075 - val_precision: 0.8002\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6096 - precision: 0.7890 - val_loss: 0.5991 - val_precision: 0.7970\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5950 - precision: 0.7947 - val_loss: 0.5956 - val_precision: 0.8051\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5918 - precision: 0.7949 - val_loss: 0.5807 - val_precision: 0.7914\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.5850 - precision: 0.7803 - val_loss: 0.5812 - val_precision: 0.7909\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5675 - precision: 0.7890 - val_loss: 0.6192 - val_precision: 0.7590\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5803 - precision: 0.7841 - val_loss: 0.5663 - val_precision: 0.7730\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5401 - precision: 0.7902 - val_loss: 0.5608 - val_precision: 0.7872\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5340 - precision: 0.8007 - val_loss: 0.5941 - val_precision: 0.7722\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5383 - precision: 0.8033 - val_loss: 0.5913 - val_precision: 0.7820\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5328 - precision: 0.8016 - val_loss: 0.6051 - val_precision: 0.7706\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.5285 - precision: 0.8030 - val_loss: 0.5639 - val_precision: 0.7652\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4970 - precision: 0.8039 - val_loss: 0.5710 - val_precision: 0.7615\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5190 - precision: 0.8078 - val_loss: 0.5771 - val_precision: 0.7701\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4977 - precision: 0.8153 - val_loss: 0.5710 - val_precision: 0.7553\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4809 - precision: 0.8098 - val_loss: 0.5549 - val_precision: 0.7706\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4845 - precision: 0.8101 - val_loss: 0.5557 - val_precision: 0.7738\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4741 - precision: 0.8128 - val_loss: 0.5618 - val_precision: 0.7726\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4697 - precision: 0.8260 - val_loss: 0.5732 - val_precision: 0.7836\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4893 - precision: 0.8290 - val_loss: 0.5684 - val_precision: 0.7727\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4631 - precision: 0.8259 - val_loss: 0.5472 - val_precision: 0.7831\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.4343 - precision: 0.8223 - val_loss: 0.5575 - val_precision: 0.7656\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4361 - precision: 0.8334 - val_loss: 0.5764 - val_precision: 0.7623\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4288 - precision: 0.8313 - val_loss: 0.5475 - val_precision: 0.7667\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4300 - precision: 0.8250 - val_loss: 0.5590 - val_precision: 0.7671\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4380 - precision: 0.8286 - val_loss: 0.5431 - val_precision: 0.7696\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.4246 - precision: 0.8382 - val_loss: 0.5522 - val_precision: 0.7870\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.4118 - precision: 0.8445 - val_loss: 0.5781 - val_precision: 0.7601\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4175 - precision: 0.8314 - val_loss: 0.6152 - val_precision: 0.7516\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4048 - precision: 0.8426 - val_loss: 0.6020 - val_precision: 0.7541\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3925 - precision: 0.8353 - val_loss: 0.5572 - val_precision: 0.7668\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.4034 - precision: 0.8406 - val_loss: 0.5567 - val_precision: 0.7600\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3892 - precision: 0.8433 - val_loss: 0.6064 - val_precision: 0.7516\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3757 - precision: 0.8450 - val_loss: 0.6119 - val_precision: 0.7523\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3795 - precision: 0.8452 - val_loss: 0.6181 - val_precision: 0.7590\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3936 - precision: 0.8413 - val_loss: 0.5663 - val_precision: 0.7672\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3765 - precision: 0.8407 - val_loss: 0.6149 - val_precision: 0.7520\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.3646 - precision: 0.8473 - val_loss: 0.6647 - val_precision: 0.7509\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3620 - precision: 0.8523 - val_loss: 0.5944 - val_precision: 0.7519\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3619 - precision: 0.8494 - val_loss: 0.5804 - val_precision: 0.7585\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3373 - precision: 0.8540 - val_loss: 0.6430 - val_precision: 0.7705\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.3492 - precision: 0.8506 - val_loss: 0.6065 - val_precision: 0.7627\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3280 - precision: 0.8647 - val_loss: 0.6121 - val_precision: 0.7498\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3408 - precision: 0.8503 - val_loss: 0.6349 - val_precision: 0.7536\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3669 - precision: 0.8450 - val_loss: 0.5802 - val_precision: 0.7802\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3353 - precision: 0.8660 - val_loss: 0.6344 - val_precision: 0.7555\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3239 - precision: 0.8602 - val_loss: 0.6085 - val_precision: 0.7691\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3155 - precision: 0.8656 - val_loss: 0.6120 - val_precision: 0.7648\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.3231 - precision: 0.8621 - val_loss: 0.6206 - val_precision: 0.7455\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.3125 - precision: 0.8652 - val_loss: 0.6581 - val_precision: 0.7466\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3186 - precision: 0.8621 - val_loss: 0.6575 - val_precision: 0.7400\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3170 - precision: 0.8626 - val_loss: 0.6474 - val_precision: 0.7543\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3082 - precision: 0.8693 - val_loss: 0.6228 - val_precision: 0.7420\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3118 - precision: 0.8669 - val_loss: 0.6743 - val_precision: 0.7511\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.3044 - precision: 0.8719 - val_loss: 0.6496 - val_precision: 0.7426\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2928 - precision: 0.8757 - val_loss: 0.6895 - val_precision: 0.7469\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2978 - precision: 0.8647 - val_loss: 0.6704 - val_precision: 0.7405\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2840 - precision: 0.8701 - val_loss: 0.8044 - val_precision: 0.7289\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.2828 - precision: 0.8799 - val_loss: 0.7354 - val_precision: 0.7300\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.2701 - precision: 0.8725 - val_loss: 0.7161 - val_precision: 0.7323\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2867 - precision: 0.8755 - val_loss: 0.6791 - val_precision: 0.7636\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.2781 - precision: 0.8725 - val_loss: 0.7278 - val_precision: 0.7428\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2709 - precision: 0.8771 - val_loss: 0.6688 - val_precision: 0.7271\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2728 - precision: 0.8724 - val_loss: 0.7166 - val_precision: 0.7359\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2761 - precision: 0.8774 - val_loss: 0.7520 - val_precision: 0.7291\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2665 - precision: 0.8873 - val_loss: 0.7388 - val_precision: 0.7351\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2574 - precision: 0.8743 - val_loss: 0.7497 - val_precision: 0.7410\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.2553 - precision: 0.8875 - val_loss: 0.7694 - val_precision: 0.7302\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2515 - precision: 0.8872 - val_loss: 0.8209 - val_precision: 0.7496\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2591 - precision: 0.8893 - val_loss: 0.7550 - val_precision: 0.7395\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2497 - precision: 0.8844 - val_loss: 0.7876 - val_precision: 0.7353\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.2521 - precision: 0.8837 - val_loss: 0.7667 - val_precision: 0.7270\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.2432 - precision: 0.8903 - val_loss: 0.8166 - val_precision: 0.7295\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2361 - precision: 0.8876 - val_loss: 0.8044 - val_precision: 0.7178\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2409 - precision: 0.8877 - val_loss: 0.8229 - val_precision: 0.7199\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2475 - precision: 0.8809 - val_loss: 0.7455 - val_precision: 0.7339\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.2441 - precision: 0.8887 - val_loss: 0.7713 - val_precision: 0.7570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a287d9f90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs =100, verbose=1, validation_data = (xtest_pad, y_test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc49c80",
   "metadata": {
    "papermill": {
     "duration": 0.640788,
     "end_time": "2022-03-11T04:48:18.361606",
     "exception": false,
     "start_time": "2022-03-11T04:48:17.720818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**A problem with training neural networks is in the choice of the number of training epochs to use.Too many epochs can lead to overfitting of the training dataset, whereas too few may result in an underfit model. Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.**\n",
    "\n",
    "**So lets use earlystopping, I see that after 43 epoch out validation loss has increased so lets use early stopping to fix that**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a532c2c",
   "metadata": {
    "papermill": {
     "duration": 0.626596,
     "end_time": "2022-03-11T04:48:19.750718",
     "exception": false,
     "start_time": "2022-03-11T04:48:19.124122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 9 LSTM word2vec + early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e04ce934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T04:48:21.052614Z",
     "iopub.status.busy": "2022-03-11T04:48:21.052006Z",
     "iopub.status.idle": "2022-03-11T05:01:45.370212Z",
     "shell.execute_reply": "2022-03-11T05:01:45.369680Z",
     "shell.execute_reply.started": "2022-03-10T16:40:32.050101Z"
    },
    "papermill": {
     "duration": 804.977576,
     "end_time": "2022-03-11T05:01:45.370362",
     "exception": false,
     "start_time": "2022-03-11T04:48:20.392786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 25s 2s/step - loss: 1.0016 - precision_1: 0.5536 - val_loss: 0.9593 - val_precision_1: 0.7619\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.9347 - precision_1: 0.6705 - val_loss: 0.8811 - val_precision_1: 0.7448\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8756 - precision_1: 0.7177 - val_loss: 0.8544 - val_precision_1: 0.7001\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8515 - precision_1: 0.7306 - val_loss: 0.8108 - val_precision_1: 0.7809\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.8117 - precision_1: 0.7441 - val_loss: 0.8801 - val_precision_1: 0.6866\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.8111 - precision_1: 0.7318 - val_loss: 0.7424 - val_precision_1: 0.7670\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7652 - precision_1: 0.7387 - val_loss: 0.7156 - val_precision_1: 0.7723\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7488 - precision_1: 0.7501 - val_loss: 0.6917 - val_precision_1: 0.7829\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7490 - precision_1: 0.7306 - val_loss: 0.7037 - val_precision_1: 0.7889\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.7330 - precision_1: 0.7388 - val_loss: 0.6972 - val_precision_1: 0.7963\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7283 - precision_1: 0.7553 - val_loss: 0.6883 - val_precision_1: 0.7897\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7224 - precision_1: 0.7402 - val_loss: 0.6707 - val_precision_1: 0.7986\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.6943 - precision_1: 0.7519 - val_loss: 0.6655 - val_precision_1: 0.7874\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6911 - precision_1: 0.7580 - val_loss: 0.6640 - val_precision_1: 0.7841\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.7071 - precision_1: 0.7492 - val_loss: 0.6628 - val_precision_1: 0.7835\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6914 - precision_1: 0.7552 - val_loss: 0.6578 - val_precision_1: 0.7927\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6699 - precision_1: 0.7590 - val_loss: 0.6565 - val_precision_1: 0.7785\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6765 - precision_1: 0.7669 - val_loss: 0.6709 - val_precision_1: 0.7719\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.6500 - precision_1: 0.7893 - val_loss: 0.6384 - val_precision_1: 0.7829\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6435 - precision_1: 0.7762 - val_loss: 0.6273 - val_precision_1: 0.8004\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6357 - precision_1: 0.7813 - val_loss: 0.6162 - val_precision_1: 0.8011\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.6281 - precision_1: 0.7747 - val_loss: 0.6067 - val_precision_1: 0.7909\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.6074 - precision_1: 0.7977 - val_loss: 0.6104 - val_precision_1: 0.7945\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.6145 - precision_1: 0.7792 - val_loss: 0.6098 - val_precision_1: 0.8031\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.6015 - precision_1: 0.7897 - val_loss: 0.6030 - val_precision_1: 0.7850\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5916 - precision_1: 0.7845 - val_loss: 0.5903 - val_precision_1: 0.7979\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5842 - precision_1: 0.7900 - val_loss: 0.5813 - val_precision_1: 0.7911\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.5736 - precision_1: 0.7868 - val_loss: 0.5921 - val_precision_1: 0.7942\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5692 - precision_1: 0.8015 - val_loss: 0.5785 - val_precision_1: 0.7780\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5441 - precision_1: 0.8083 - val_loss: 0.5841 - val_precision_1: 0.7990\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.5552 - precision_1: 0.7961 - val_loss: 0.5618 - val_precision_1: 0.7689\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 22s 2s/step - loss: 0.5430 - precision_1: 0.7940 - val_loss: 0.5708 - val_precision_1: 0.8004\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5150 - precision_1: 0.8081 - val_loss: 0.5590 - val_precision_1: 0.7787\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 21s 2s/step - loss: 0.5185 - precision_1: 0.8061 - val_loss: 0.5692 - val_precision_1: 0.7666\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5349 - precision_1: 0.7908 - val_loss: 0.5869 - val_precision_1: 0.7784\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 23s 2s/step - loss: 0.5214 - precision_1: 0.8187 - val_loss: 0.5817 - val_precision_1: 0.7739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a01921850>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [tf.keras.metrics.Precision()])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c5906",
   "metadata": {
    "papermill": {
     "duration": 0.759834,
     "end_time": "2022-03-11T05:01:46.900547",
     "exception": false,
     "start_time": "2022-03-11T05:01:46.140713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Precision is close to 80 now, so there is an improvment in precision score. Lets see the classification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "260720f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T05:01:48.453401Z",
     "iopub.status.busy": "2022-03-11T05:01:48.452370Z",
     "iopub.status.idle": "2022-03-11T05:02:50.543917Z",
     "shell.execute_reply": "2022-03-11T05:02:50.544765Z",
     "shell.execute_reply.started": "2022-03-10T16:52:57.844143Z"
    },
    "papermill": {
     "duration": 62.877913,
     "end_time": "2022-03-11T05:02:50.545012",
     "exception": false,
     "start_time": "2022-03-11T05:01:47.667099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       620\n",
      "           1       0.84      0.68      0.75       381\n",
      "           2       0.48      0.54      0.51       168\n",
      "\n",
      "    accuracy                           0.73      1169\n",
      "   macro avg       0.69      0.68      0.68      1169\n",
      "weighted avg       0.74      0.73      0.73      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(xtest_pad)\n",
    "\n",
    "# Here '2' is '-1' in previous reports\n",
    "\n",
    "print(classification_report(np.argmax(y_test_enc,axis=1), np.argmax(y_pred,axis=1), labels=[0,1,2]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe22af",
   "metadata": {
    "papermill": {
     "duration": 0.770181,
     "end_time": "2022-03-11T05:02:52.140439",
     "exception": false,
     "start_time": "2022-03-11T05:02:51.370258",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Its evident that there is a improvement in all the clasification classes, hence this will be our new baseline model.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee07c3",
   "metadata": {
    "papermill": {
     "duration": 0.771795,
     "end_time": "2022-03-11T05:02:53.681329",
     "exception": false,
     "start_time": "2022-03-11T05:02:52.909534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 10 bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fac3594c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T05:02:55.232952Z",
     "iopub.status.busy": "2022-03-11T05:02:55.232041Z",
     "iopub.status.idle": "2022-03-11T05:30:30.271884Z",
     "shell.execute_reply": "2022-03-11T05:30:30.271380Z",
     "shell.execute_reply.started": "2022-03-10T16:53:00.923954Z"
    },
    "papermill": {
     "duration": 1655.820221,
     "end_time": "2022-03-11T05:30:30.272033",
     "exception": false,
     "start_time": "2022-03-11T05:02:54.451812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 51s 5s/step - loss: 0.9972 - precision_2: 0.5596 - val_loss: 0.9385 - val_precision_2: 0.7343\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.9178 - precision_2: 0.6558 - val_loss: 0.8764 - val_precision_2: 0.7553\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.8695 - precision_2: 0.7328 - val_loss: 0.8563 - val_precision_2: 0.8256\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.8435 - precision_2: 0.7347 - val_loss: 0.8081 - val_precision_2: 0.7496\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.8205 - precision_2: 0.7220 - val_loss: 0.7833 - val_precision_2: 0.7627\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.7942 - precision_2: 0.7298 - val_loss: 0.7921 - val_precision_2: 0.7986\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.7789 - precision_2: 0.7394 - val_loss: 0.7292 - val_precision_2: 0.7851\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.7633 - precision_2: 0.7342 - val_loss: 0.7085 - val_precision_2: 0.7754\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.7538 - precision_2: 0.7277 - val_loss: 0.7189 - val_precision_2: 0.7574\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.7422 - precision_2: 0.7282 - val_loss: 0.6993 - val_precision_2: 0.7746\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.7141 - precision_2: 0.7492 - val_loss: 0.7223 - val_precision_2: 0.7453\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.7279 - precision_2: 0.7427 - val_loss: 0.6869 - val_precision_2: 0.7964\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.7036 - precision_2: 0.7559 - val_loss: 0.6970 - val_precision_2: 0.7657\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.7019 - precision_2: 0.7585 - val_loss: 0.6700 - val_precision_2: 0.7835\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.6868 - precision_2: 0.7555 - val_loss: 0.6944 - val_precision_2: 0.7527\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.7136 - precision_2: 0.7396 - val_loss: 0.6679 - val_precision_2: 0.7754\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6992 - precision_2: 0.7420 - val_loss: 0.7055 - val_precision_2: 0.7520\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6991 - precision_2: 0.7560 - val_loss: 0.6701 - val_precision_2: 0.7959\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.6588 - precision_2: 0.7666 - val_loss: 0.6406 - val_precision_2: 0.7695\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6561 - precision_2: 0.7592 - val_loss: 0.6229 - val_precision_2: 0.7844\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6362 - precision_2: 0.7726 - val_loss: 0.6337 - val_precision_2: 0.7905\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.6236 - precision_2: 0.7837 - val_loss: 0.6689 - val_precision_2: 0.7497\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.6525 - precision_2: 0.7683 - val_loss: 0.6199 - val_precision_2: 0.7845\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.6323 - precision_2: 0.7600 - val_loss: 0.7235 - val_precision_2: 0.7268\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6489 - precision_2: 0.7749 - val_loss: 0.6138 - val_precision_2: 0.8020\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.6383 - precision_2: 0.7752 - val_loss: 0.6058 - val_precision_2: 0.8098\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.6213 - precision_2: 0.7728 - val_loss: 0.5951 - val_precision_2: 0.7671\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.5913 - precision_2: 0.7802 - val_loss: 0.6429 - val_precision_2: 0.7533\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.5913 - precision_2: 0.7758 - val_loss: 0.5965 - val_precision_2: 0.7756\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.5747 - precision_2: 0.7896 - val_loss: 0.5759 - val_precision_2: 0.7953\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.5600 - precision_2: 0.7922 - val_loss: 0.5659 - val_precision_2: 0.7805\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.5472 - precision_2: 0.7945 - val_loss: 0.5778 - val_precision_2: 0.7653\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.5325 - precision_2: 0.8066 - val_loss: 0.5528 - val_precision_2: 0.7742\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 45s 4s/step - loss: 0.5220 - precision_2: 0.8025 - val_loss: 0.5457 - val_precision_2: 0.7924\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.5189 - precision_2: 0.8059 - val_loss: 0.5502 - val_precision_2: 0.7900\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 43s 4s/step - loss: 0.5189 - precision_2: 0.8039 - val_loss: 0.5584 - val_precision_2: 0.7879\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 44s 4s/step - loss: 0.4855 - precision_2: 0.8237 - val_loss: 0.5739 - val_precision_2: 0.7909\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "model_bi.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], trainable = False))\n",
    "\n",
    "model_bi.add(SpatialDropout1D(0.3))\n",
    "model_bi.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\n",
    "\n",
    "model_bi.add(Dense(1024, activation = 'relu'))\n",
    "model_bi.add(Dropout(0.8))\n",
    "\n",
    "model_bi.add(Dense(1024, activation = 'relu'))\n",
    "model_bi.add(Dropout(0.8))\n",
    "\n",
    "model_bi.add(Dense(3))\n",
    "model_bi.add(Activation('softmax'))\n",
    "model_bi.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = tf.keras.metrics.Precision())\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 0, mode = 'auto')\n",
    "model_bi.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])\n",
    "\n",
    "y_pred = model_bi.predict(xtest_pad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "815c6edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T05:30:32.157715Z",
     "iopub.status.busy": "2022-03-11T05:30:32.156756Z",
     "iopub.status.idle": "2022-03-11T05:30:32.166562Z",
     "shell.execute_reply": "2022-03-11T05:30:32.166037Z",
     "shell.execute_reply.started": "2022-03-10T17:09:36.061491Z"
    },
    "papermill": {
     "duration": 0.935067,
     "end_time": "2022-03-11T05:30:32.166705",
     "exception": false,
     "start_time": "2022-03-11T05:30:31.231638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81       620\n",
      "           1       0.81      0.70      0.75       381\n",
      "           2       0.56      0.30      0.39       168\n",
      "\n",
      "    accuracy                           0.75      1169\n",
      "   macro avg       0.70      0.63      0.65      1169\n",
      "weighted avg       0.74      0.75      0.73      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9daab2",
   "metadata": {
    "papermill": {
     "duration": 0.944629,
     "end_time": "2022-03-11T05:30:34.090990",
     "exception": false,
     "start_time": "2022-03-11T05:30:33.146361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Well bidrectional LSTM did not beat our baseline LSTM model so we are not going to consider this model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd938752",
   "metadata": {
    "papermill": {
     "duration": 0.912429,
     "end_time": "2022-03-11T05:30:35.930015",
     "exception": false,
     "start_time": "2022-03-11T05:30:35.017586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 11 GRU 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e031511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T05:30:38.744816Z",
     "iopub.status.busy": "2022-03-11T05:30:38.743865Z",
     "iopub.status.idle": "2022-03-11T05:41:17.009030Z",
     "shell.execute_reply": "2022-03-11T05:41:17.009750Z",
     "shell.execute_reply.started": "2022-03-10T17:09:36.076012Z"
    },
    "papermill": {
     "duration": 639.786602,
     "end_time": "2022-03-11T05:41:17.009955",
     "exception": false,
     "start_time": "2022-03-11T05:30:37.223353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 41s 4s/step - loss: 1.0093 - precision_3: 0.5587 - val_loss: 0.9452 - val_precision_3: 0.7292\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.9217 - precision_3: 0.6558 - val_loss: 0.8765 - val_precision_3: 0.7454\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.8808 - precision_3: 0.7214 - val_loss: 0.8542 - val_precision_3: 0.7607\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.8646 - precision_3: 0.7121 - val_loss: 0.8230 - val_precision_3: 0.7401\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.8346 - precision_3: 0.7054 - val_loss: 0.7885 - val_precision_3: 0.7903\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.8001 - precision_3: 0.7233 - val_loss: 0.7386 - val_precision_3: 0.7672\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 38s 4s/step - loss: 0.7966 - precision_3: 0.7212 - val_loss: 0.8118 - val_precision_3: 0.7295\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.8032 - precision_3: 0.7357 - val_loss: 0.7592 - val_precision_3: 0.7454\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7754 - precision_3: 0.7364 - val_loss: 0.7327 - val_precision_3: 0.7768\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7603 - precision_3: 0.7293 - val_loss: 0.7215 - val_precision_3: 0.8135\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.7541 - precision_3: 0.7344 - val_loss: 0.7406 - val_precision_3: 0.7893\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7493 - precision_3: 0.7509 - val_loss: 0.7012 - val_precision_3: 0.7786\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7369 - precision_3: 0.7522 - val_loss: 0.7059 - val_precision_3: 0.7985\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.7181 - precision_3: 0.7614 - val_loss: 0.6670 - val_precision_3: 0.7845\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7331 - precision_3: 0.7385 - val_loss: 0.7128 - val_precision_3: 0.7730\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 0.7099 - precision_3: 0.7458 - val_loss: 0.6899 - val_precision_3: 0.8143\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 36s 4s/step - loss: 0.7206 - precision_3: 0.7466 - val_loss: 0.7173 - val_precision_3: 0.7623\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77       620\n",
      "           1       0.86      0.46      0.60       381\n",
      "           2       0.37      0.57      0.45       168\n",
      "\n",
      "    accuracy                           0.67      1169\n",
      "   macro avg       0.65      0.62      0.60      1169\n",
      "weighted avg       0.72      0.67      0.67      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(len(word_index)+1, 300, weights = [embedding_matrix], trainable=False))\n",
    "\n",
    "model_gru.add(SpatialDropout1D(0.3))\n",
    "model_gru.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences = True))\n",
    "model_gru.add(GRU(300, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "\n",
    "model_gru.add(Dense(1024, activation = 'relu'))\n",
    "model_gru.add(Dropout(0.8))\n",
    "\n",
    "model_gru.add(Dense(1024, activation = 'relu'))\n",
    "model_gru.add(Dropout(0.8))\n",
    "\n",
    "model_gru.add(Dense(3, activation = 'softmax'))\n",
    "model_gru.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = tf.keras.metrics.Precision())\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 0, mode= 'auto')\n",
    "\n",
    "model_gru.fit(xtrain_pad, y=y_train_enc, batch_size = 512, epochs = 100, verbose=1, validation_data = (xtest_pad, y_test_enc),callbacks = [earlystop])\n",
    "\n",
    "y_pred = model_gru.predict(xtest_pad)\n",
    "\n",
    "print(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred , axis=1), labels = [0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba6435",
   "metadata": {
    "papermill": {
     "duration": 0.960355,
     "end_time": "2022-03-11T05:41:18.973182",
     "exception": false,
     "start_time": "2022-03-11T05:41:18.012827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Well not much difference between baseline single layer LSTM model to GRU model, but it looks our baseline model is still better.**\n",
    "\n",
    "**Lets add one more LSTM layer to baseline model and see what happens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f9935",
   "metadata": {
    "papermill": {
     "duration": 0.998845,
     "end_time": "2022-03-11T05:41:20.932635",
     "exception": false,
     "start_time": "2022-03-11T05:41:19.933790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 12 LSTM multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dd28110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T05:41:22.895963Z",
     "iopub.status.busy": "2022-03-11T05:41:22.894930Z",
     "iopub.status.idle": "2022-03-11T05:54:41.788143Z",
     "shell.execute_reply": "2022-03-11T05:54:41.788908Z",
     "shell.execute_reply.started": "2022-03-10T17:17:09.017632Z"
    },
    "papermill": {
     "duration": 799.859504,
     "end_time": "2022-03-11T05:54:41.789138",
     "exception": false,
     "start_time": "2022-03-11T05:41:21.929634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 53s 5s/step - loss: 1.0124 - precision_4: 0.5763 - val_loss: 0.9283 - val_precision_4: 0.6295\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.9200 - precision_4: 0.6727 - val_loss: 0.8817 - val_precision_4: 0.6846\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.8736 - precision_4: 0.7307 - val_loss: 0.8506 - val_precision_4: 0.8113\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 47s 5s/step - loss: 0.8362 - precision_4: 0.7200 - val_loss: 0.7787 - val_precision_4: 0.7817\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.7802 - precision_4: 0.7487 - val_loss: 0.8040 - val_precision_4: 0.7574\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.7960 - precision_4: 0.7314 - val_loss: 0.7376 - val_precision_4: 0.7785\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 47s 5s/step - loss: 0.7722 - precision_4: 0.7341 - val_loss: 0.7076 - val_precision_4: 0.7844\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.7631 - precision_4: 0.7286 - val_loss: 0.7237 - val_precision_4: 0.7753\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.7399 - precision_4: 0.7422 - val_loss: 0.7258 - val_precision_4: 0.7494\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.7214 - precision_4: 0.7480 - val_loss: 0.6800 - val_precision_4: 0.7965\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 48s 5s/step - loss: 0.7179 - precision_4: 0.7567 - val_loss: 0.7094 - val_precision_4: 0.7917\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.7308 - precision_4: 0.7350 - val_loss: 0.7322 - val_precision_4: 0.7973\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.7200 - precision_4: 0.7533 - val_loss: 0.6667 - val_precision_4: 0.7810\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 48s 5s/step - loss: 0.6820 - precision_4: 0.7637 - val_loss: 0.6378 - val_precision_4: 0.8011\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.6818 - precision_4: 0.7538 - val_loss: 0.6562 - val_precision_4: 0.7707\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 46s 5s/step - loss: 0.6657 - precision_4: 0.7662 - val_loss: 0.6648 - val_precision_4: 0.7863\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 45s 5s/step - loss: 0.6637 - precision_4: 0.7772 - val_loss: 0.6487 - val_precision_4: 0.7817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78       620\n",
      "           1       0.88      0.51      0.65       381\n",
      "           2       0.45      0.35      0.39       168\n",
      "\n",
      "    accuracy                           0.70      1169\n",
      "   macro avg       0.67      0.59      0.61      1169\n",
      "weighted avg       0.72      0.70      0.68      1169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3 ,  return_sequences = True))\n",
    "model.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = [tf.keras.metrics.Precision()])\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=y_train_enc, batch_size=512, epochs=100, \n",
    "          verbose=1, validation_data = (xtest_pad, y_test_enc), callbacks=[earlystop])\n",
    "\n",
    "y_pred = model.predict(xtest_pad)\n",
    "print(classification_report(np.argmax(y_test_enc, axis=1), np.argmax(y_pred, axis=1), labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940cab5",
   "metadata": {
    "papermill": {
     "duration": 1.015379,
     "end_time": "2022-03-11T05:54:43.879634",
     "exception": false,
     "start_time": "2022-03-11T05:54:42.864255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Well the additional LSTM layer has imporoved the first 2 labels and did not imporove much for third label**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe861e",
   "metadata": {
    "papermill": {
     "duration": 1.036075,
     "end_time": "2022-03-11T05:54:45.987510",
     "exception": false,
     "start_time": "2022-03-11T05:54:44.951435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**I will stop this notebook here, it has become very big already, maybe i will create an other notebook only for transfer learning! anyway there are many ways to model in machine learning, it depends on your dataset which model suits the best.** \n",
    "\n",
    "**Until then keep hustling! See you next time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ee8f4",
   "metadata": {
    "papermill": {
     "duration": 1.114262,
     "end_time": "2022-03-11T05:54:48.144411",
     "exception": false,
     "start_time": "2022-03-11T05:54:47.030149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7581.853641,
   "end_time": "2022-03-11T05:54:52.798035",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-11T03:48:30.944394",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
